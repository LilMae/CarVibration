{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import  DataLoader\n",
    "from newdataset import UntrimmedDataset\n",
    "\n",
    "data_root = os.path.join(os.getcwd(), 'Untrimmed_accline')\n",
    "n_fft = 32\n",
    "hop_length = int(n_fft/4)\n",
    "device = 'cpu'\n",
    "feature_size = 32\n",
    "\n",
    "squeeze_net = torchvision.models.squeezenet1_1(progress=True).to(device)\n",
    "squeeze_net.classifier = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=(1,1)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(512, feature_size, bias=True))\n",
    "backbone =  squeeze_net\n",
    "\n",
    "dataset = UntrimmedDataset(root_dir=data_root,\n",
    "                               kernel_size=1024,\n",
    "                               stride=256,\n",
    "                               device='cpu',\n",
    "                               n_fft=n_fft,\n",
    "                               hop_length=hop_length)\n",
    "    \n",
    "dataloader = DataLoader(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "torch.Size([3, 17, 129])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(len(dataset))\n",
    "\n",
    "for stft, class_info, is_new in dataloader:\n",
    "    a_stft = stft[2]\n",
    "    # print(a_stft)\n",
    "    break\n",
    "print(a_stft.shape)\n",
    "\n",
    "new_stft = []\n",
    "for ch in a_stft:\n",
    "    new_stft.append(torch.abs(ch))\n",
    "new_stft = torch.stack(new_stft)\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 이미지 크기를 256x256으로 조정\n",
    "])\n",
    "\n",
    "new_stft = transform(new_stft)\n",
    "\n",
    "tensor_rgb = new_stft.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tensor_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/lilmae/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:528: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343680142/work/aten/src/ATen/native/Copy.cpp:276.)\n",
      "  img = img.to(req_dtype)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input tensor should be a float tensor. Got torch.complex128.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)),  \u001b[38;5;66;03m# 이미지 크기를 256x256으로 조정\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),  \u001b[38;5;66;03m# 50% 확률로 수평 반전\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# 정규화\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stft, class_info, is_new \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 10\u001b[0m     stft \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     res \u001b[38;5;241m=\u001b[39m squeeze_net(stft)\n\u001b[1;32m     12\u001b[0m     a_res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:909\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    906\u001b[0m _assert_image_tensor(tensor)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor should be a float tensor. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Input tensor should be a float tensor. Got torch.complex128."
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, utils\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 이미지 크기를 256x256으로 조정\n",
    "    transforms.RandomHorizontalFlip(),  # 50% 확률로 수평 반전\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
    "])\n",
    "\n",
    "for stft, class_info, is_new in dataloader:\n",
    "    stft = transform(stft)\n",
    "    res = squeeze_net(stft)\n",
    "    a_res = res[0]\n",
    "    print(a_res.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "batch = torch.randn([10, 32])\n",
    "\n",
    "norm_batch = F.normalize(batch, p=2,dim=1)\n",
    "sim_matrix = torch.mm(norm_batch, norm_batch.t())\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for i in range(batch.shape[0]):\n",
    "    for j in range(batch.shape[0]):\n",
    "        if j==i+1:\n",
    "            pos.append(sim_matrix[i][j])\n",
    "        elif j>i:\n",
    "            neg.append(sim_matrix[i][j])\n",
    "pos = torch.stack(pos)\n",
    "neg = torch.stack(neg)\n",
    "print(f'pos_sum : {pos}')\n",
    "print(f'neg_sum : {neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nce_loss_fn(history, future, similarity, temperature='0.1'):\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    N = history.shape[0]\n",
    "    \n",
    "    \n",
    "    sim = similarity(history, future)\n",
    "    \n",
    "    \n",
    "    pos_sim = K.exp(tf.linalg.tensor_diag_part(sim)/temperature)\n",
    "\n",
    "    tri_mask = np.ones(N ** 2, dtype=np.bool).reshape(N, N)\n",
    "    tri_mask[np.diag_indices(N)] = False\n",
    "    neg = tf.reshape(tf.boolean_mask(sim, tri_mask), [N, N - 1])\n",
    "    all_sim = K.exp(sim/temperature)\n",
    "\n",
    "    logits = tf.divide(K.sum(pos_sim), K.sum(all_sim, axis=1))\n",
    "\n",
    "    lbl = np.ones(history.shape[0])\n",
    "    # categorical cross entropy\n",
    "    loss = criterion(y_pred = logits, y_true = lbl)\n",
    "    # loss = K.sum(logits)\n",
    "    # divide by the size of batch\n",
    "    #loss = loss / lbl.shape[0]\n",
    "    # similarity of positive pairs (only for debug)\n",
    "    mean_sim = K.mean(tf.linalg.tensor_diag_part(sim))\n",
    "    mean_neg = K.mean(neg)\n",
    "    return loss, mean_sim, mean_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 임의의 10개 텐서 생성 (예시로 각각 128 차원)\n",
    "tensors = [torch.randn(128) for _ in range(10)]\n",
    "\n",
    "# 모든 텐서를 하나의 배치로 쌓기\n",
    "tensor_stack = torch.stack(tensors)\n",
    "\n",
    "# L2 정규화 (코사인 유사도를 위해 필요)\n",
    "tensor_norm = F.normalize(tensor_stack, p=2, dim=1)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "cosine_similarities = torch.mm(tensor_norm, tensor_norm.t())\n",
    "\n",
    "# 유사도 행렬 출력\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
