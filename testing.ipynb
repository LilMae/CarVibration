{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::complex<double>) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     29\u001b[0m cpd \u001b[38;5;241m=\u001b[39m CPD_SSL(backbone\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSqeezeNet\u001b[39m\u001b[38;5;124m'\u001b[39m, feature_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mcpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/CarVibration/model.py:63\u001b[0m, in \u001b[0;36mCPD_SSL.train\u001b[0;34m(self, train_loader, epoch)\u001b[0m\n\u001b[1;32m     60\u001b[0m optimier \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch):\n\u001b[0;32m---> 63\u001b[0m     loss_epoch, mean_pos_epoch, mean_neg_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimier\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | loss_epoch : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | mean_pos : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_pos_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | mean_neg : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_neg_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/CarVibration/model.py:80\u001b[0m, in \u001b[0;36mCPD_SSL.train_one_epoch\u001b[0;34m(self, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     76\u001b[0m data_stft, class_tensor, is_new \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     78\u001b[0m data_stft\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 80\u001b[0m loss_step, mean_pos, mean_neg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_stft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m loss_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_step\n\u001b[1;32m     83\u001b[0m mean_pos_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m mean_pos\n",
      "File \u001b[0;32m~/Desktop/CarVibration/model.py:97\u001b[0m, in \u001b[0;36mCPD_SSL.train_one_step\u001b[0;34m(self, batch, optimizer)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_one_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, optimizer):\n\u001b[1;32m     95\u001b[0m     loss \u001b[38;5;241m=\u001b[39m InfoNCE(negative_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpair\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 97\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m     query \u001b[38;5;241m=\u001b[39m batch[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    100\u001b[0m     positive_pair \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/models/squeezenet.py:95\u001b[0m, in \u001b[0;36mSqueezeNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 95\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (c10::complex<double>) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import  DataLoader\n",
    "from newdataset import UntrimmedDataset\n",
    "from model import CPD_SSL\n",
    "data_root = os.path.join(os.getcwd(), 'Untrimmed_accline')\n",
    "n_fft = 32\n",
    "hop_length = int(n_fft/2)\n",
    "device = 'cpu'\n",
    "feature_size = 32\n",
    "\n",
    "squeeze_net = torchvision.models.squeezenet1_1(progress=True).to(device)\n",
    "squeeze_net.classifier = torch.nn.Sequential(\n",
    "    torch.nn.AdaptiveAvgPool2d(output_size=(1,1)),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(512, feature_size, bias=True))\n",
    "backbone =  squeeze_net\n",
    "\n",
    "dataset = UntrimmedDataset(root_dir=data_root,\n",
    "                               kernel_size=1024,\n",
    "                               stride=256,\n",
    "                               device='cpu',\n",
    "                               n_fft=n_fft,\n",
    "                               hop_length=hop_length)\n",
    "    \n",
    "dataloader = DataLoader(dataset, batch_size=4)\n",
    "\n",
    "cpd = CPD_SSL(backbone='SqeezeNet', feature_size=32, device='cpu')\n",
    "cpd.train(dataloader, epoch=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n",
      "torch.Size([3, 17, 65])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(len(dataset))\n",
    "\n",
    "for stft, class_info, is_new in dataloader:\n",
    "    a_stft = stft[2]\n",
    "    # print(a_stft)\n",
    "    break\n",
    "print(a_stft.shape)\n",
    "\n",
    "new_stft = []\n",
    "for ch in a_stft:\n",
    "    new_stft.append(torch.abs(ch))\n",
    "new_stft = torch.stack(new_stft)\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import transforms, utils\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 이미지 크기를 256x256으로 조정\n",
    "])\n",
    "\n",
    "new_stft = transform(new_stft)\n",
    "\n",
    "tensor_rgb = new_stft.permute(1, 2, 0)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(tensor_rgb)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_stft = torch.stft(input=torch.tensor(x_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_stft = torch.stft(input=torch.tensor(y_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/Desktop/CarVibration/newdataset.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_stft = torch.stft(input=torch.tensor(z_data),n_fft=self.n_fft,hop_length=self.hop_length,return_complex=True)\n",
      "/Users/lilmae/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/Users/lilmae/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:528: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1682343680142/work/aten/src/ATen/native/Copy.cpp:276.)\n",
      "  img = img.to(req_dtype)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input tensor should be a float tensor. Got torch.complex128.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m)),  \u001b[38;5;66;03m# 이미지 크기를 256x256으로 조정\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),  \u001b[38;5;66;03m# 50% 확률로 수평 반전\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# 정규화\u001b[39;00m\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m stft, class_info, is_new \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m---> 10\u001b[0m     stft \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstft\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     res \u001b[38;5;241m=\u001b[39m squeeze_net(stft)\n\u001b[1;32m     12\u001b[0m     a_res \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/functional.py:363\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vibration/lib/python3.8/site-packages/torchvision/transforms/_functional_tensor.py:909\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    906\u001b[0m _assert_image_tensor(tensor)\n\u001b[1;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mis_floating_point():\n\u001b[0;32m--> 909\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor should be a float tensor. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensor\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    913\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: Input tensor should be a float tensor. Got torch.complex128."
     ]
    }
   ],
   "source": [
    "from torchvision import transforms, utils\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # 이미지 크기를 256x256으로 조정\n",
    "    transforms.RandomHorizontalFlip(),  # 50% 확률로 수평 반전\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 정규화\n",
    "])\n",
    "def sampler(batch):\n",
    "    query = batch[:-1]\n",
    "    positive_pair = batch[1:]\n",
    "    negative_pair = []\n",
    "\n",
    "    for i in range(len(batch)-1):\n",
    "        neg_i = []\n",
    "        for j in range(len(batch)):\n",
    "            if i!=j and j!=i+1:\n",
    "                neg_i.append(batch[j])\n",
    "        neg_i = torch.stack(neg_i)\n",
    "        negative_pair.append(neg_i)\n",
    "\n",
    "    negative_pair = torch.stack(negative_pair)\n",
    "    \n",
    "    return query, positive_pair, negative_pair\n",
    "\n",
    "\n",
    "for stft, class_info, is_new in dataloader:\n",
    "    stft = transform(stft)\n",
    "    res = squeeze_net(stft)\n",
    "    a_res = res[0]\n",
    "    print(a_res.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_sum : 6.351304531097412\n",
      "neg_sum : 25.342113494873047\n",
      "loss : 31.693418502807617 pos_mean : 0.7057005167007446 neg_mean : 0.7039476037025452\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "batch = torch.randn([10, 32])\n",
    "\n",
    "norm_batch = F.normalize(batch, p=2,dim=1)\n",
    "sim_matrix = torch.mm(norm_batch, norm_batch.t())\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for i in range(batch.shape[0]):\n",
    "    for j in range(batch.shape[0]):\n",
    "        if j==i+1:\n",
    "            pos.append(sim_matrix[i][j])\n",
    "        elif j>i:\n",
    "            neg.append(sim_matrix[i][j])\n",
    "pos = torch.stack(pos)\n",
    "pos_label = torch.ones_like(pos)\n",
    "neg = torch.stack(neg)\n",
    "neg_label = torch.zeros_like(neg)\n",
    "\n",
    "pos_loss = criterion(pos, pos_label)\n",
    "neg_loss = criterion(neg, neg_label)\n",
    "\n",
    "print(f'pos_sum : {pos_loss}')\n",
    "print(f'neg_sum : {neg_loss}')\n",
    "\n",
    "loss = pos_loss+neg_loss\n",
    "\n",
    "pos_loss_mean = pos_loss/len(pos)\n",
    "neg_loss_mean = neg_loss/len(neg)\n",
    "\n",
    "print(f'loss : {loss} pos_mean : {pos_loss_mean} neg_mean : {neg_loss_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nce_loss_fn(history, future, similarity, temperature='0.1'):\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "    N = history.shape[0]\n",
    "    \n",
    "    \n",
    "    sim = similarity(history, future)\n",
    "    \n",
    "    \n",
    "    pos_sim = K.exp(tf.linalg.tensor_diag_part(sim)/temperature)\n",
    "\n",
    "    tri_mask = np.ones(N ** 2, dtype=np.bool).reshape(N, N)\n",
    "    tri_mask[np.diag_indices(N)] = False\n",
    "    neg = tf.reshape(tf.boolean_mask(sim, tri_mask), [N, N - 1])\n",
    "    all_sim = K.exp(sim/temperature)\n",
    "\n",
    "    logits = tf.divide(K.sum(pos_sim), K.sum(all_sim, axis=1))\n",
    "\n",
    "    lbl = np.ones(history.shape[0])\n",
    "    # categorical cross entropy\n",
    "    loss = criterion(y_pred = logits, y_true = lbl)\n",
    "    # loss = K.sum(logits)\n",
    "    # divide by the size of batch\n",
    "    #loss = loss / lbl.shape[0]\n",
    "    # similarity of positive pairs (only for debug)\n",
    "    mean_sim = K.mean(tf.linalg.tensor_diag_part(sim))\n",
    "    mean_neg = K.mean(neg)\n",
    "    return loss, mean_sim, mean_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 임의의 10개 텐서 생성 (예시로 각각 128 차원)\n",
    "tensors = [torch.randn(128) for _ in range(10)]\n",
    "\n",
    "# 모든 텐서를 하나의 배치로 쌓기\n",
    "tensor_stack = torch.stack(tensors)\n",
    "\n",
    "# L2 정규화 (코사인 유사도를 위해 필요)\n",
    "tensor_norm = F.normalize(tensor_stack, p=2, dim=1)\n",
    "\n",
    "# 코사인 유사도 계산\n",
    "cosine_similarities = torch.mm(tensor_norm, tensor_norm.t())\n",
    "\n",
    "# 유사도 행렬 출력\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "batch = torch.randn([10, 32])\n",
    "\n",
    "norm_batch = F.normalize(batch, p=2,dim=1)\n",
    "sim_matrix = torch.mm(norm_batch, norm_batch.t())\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "pos = []\n",
    "neg = []\n",
    "for i in range(batch.shape[0]):\n",
    "    for j in range(batch.shape[0]):\n",
    "        if j==i+1:\n",
    "            pos.append(sim_matrix[i][j])\n",
    "        elif j>i:\n",
    "            neg.append(sim_matrix[i][j])\n",
    "pos = torch.stack(pos)\n",
    "pos_label = torch.ones_like(pos)\n",
    "neg = torch.stack(neg)\n",
    "neg_label = torch.zeros_like(neg)\n",
    "\n",
    "pos_loss = criterion(pos, pos_label)\n",
    "neg_loss = criterion(neg, neg_label)\n",
    "\n",
    "print(f'pos_sum : {pos_loss}')\n",
    "print(f'neg_sum : {neg_loss}')\n",
    "\n",
    "loss = pos_loss+neg_loss\n",
    "\n",
    "pos_loss_mean = pos_loss/len(pos)\n",
    "neg_loss_mean = neg_loss/len(neg)\n",
    "\n",
    "print(f'loss : {loss} pos_mean : {pos_loss_mean} neg_mean : {neg_loss_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00,  2.6138e-01, -4.6458e-02, -2.9407e-01,  1.0684e-01,\n",
      "         -3.1089e-01, -2.2795e-01, -1.1278e-01, -2.9978e-02,  3.3109e-01],\n",
      "        [ 2.6138e-01,  1.0000e+00,  1.5032e-03,  1.9884e-01, -2.7683e-01,\n",
      "         -1.2667e-01,  9.4905e-02,  1.2522e-01, -6.4175e-02,  8.0719e-02],\n",
      "        [-4.6458e-02,  1.5032e-03,  1.0000e+00,  8.2984e-02,  3.2828e-01,\n",
      "          2.4155e-02,  2.6219e-01, -8.1057e-03, -7.9073e-04, -1.6928e-01],\n",
      "        [-2.9407e-01,  1.9884e-01,  8.2984e-02,  1.0000e+00, -2.0992e-01,\n",
      "          3.3641e-02,  2.6952e-01, -1.1424e-01, -2.1629e-01, -1.9629e-02],\n",
      "        [ 1.0684e-01, -2.7683e-01,  3.2828e-01, -2.0992e-01,  1.0000e+00,\n",
      "          1.3096e-01,  2.2677e-02, -3.7913e-01, -1.4136e-02, -2.0485e-01],\n",
      "        [-3.1089e-01, -1.2667e-01,  2.4155e-02,  3.3641e-02,  1.3096e-01,\n",
      "          1.0000e+00,  6.1755e-02, -2.1334e-01, -1.3944e-01, -1.4275e-01],\n",
      "        [-2.2795e-01,  9.4905e-02,  2.6219e-01,  2.6952e-01,  2.2677e-02,\n",
      "          6.1755e-02,  1.0000e+00, -1.2033e-02,  9.5358e-02, -4.3723e-01],\n",
      "        [-1.1278e-01,  1.2522e-01, -8.1057e-03, -1.1424e-01, -3.7913e-01,\n",
      "         -2.1334e-01, -1.2033e-02,  1.0000e+00,  3.9523e-01,  9.2278e-02],\n",
      "        [-2.9978e-02, -6.4175e-02, -7.9074e-04, -2.1629e-01, -1.4136e-02,\n",
      "         -1.3944e-01,  9.5358e-02,  3.9523e-01,  1.0000e+00, -1.4189e-01],\n",
      "        [ 3.3109e-01,  8.0719e-02, -1.6928e-01, -1.9629e-02, -2.0485e-01,\n",
      "         -1.4275e-01, -4.3723e-01,  9.2278e-02, -1.4189e-01,  1.0000e+00]])\n",
      "tensor([22026.4648, 22026.4648, 22026.4453, 22026.4453, 22026.4238, 22026.4648,\n",
      "        22026.4648, 22026.4453, 22026.4648, 22026.4863])\n",
      "loss : 0.00046192528679966927, mean_sim : 1.0, mean_neg : -0.020296189934015274\n"
     ]
    }
   ],
   "source": [
    "temperature = 0.1\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "batch = torch.randn([10, 32])\n",
    "N= 10\n",
    "\n",
    "norm_batch = F.normalize(batch, p=2,dim=1)\n",
    "sim = torch.mm(norm_batch, norm_batch.t())\n",
    "print(sim)\n",
    "pos_sim = torch.exp(torch.diag(sim) / temperature)\n",
    "print(pos_sim)\n",
    "\n",
    "tri_mask = torch.ones(N, N, dtype=torch.bool)\n",
    "tri_mask.fill_diagonal_(False)\n",
    "neg = sim.masked_select(tri_mask).view(N, N - 1)\n",
    "all_sim = torch.exp(sim / temperature)\n",
    "\n",
    "logits = torch.sum(pos_sim) / torch.sum(all_sim, dim=1)\n",
    "\n",
    "lbl = torch.ones(N)\n",
    "loss = criterion(logits, lbl)\n",
    "\n",
    "mean_sim = torch.mean(torch.diag(sim))\n",
    "mean_neg = torch.mean(neg)\n",
    "print(f'loss : {loss}, mean_sim : {mean_sim}, mean_neg : {mean_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "__all__ = ['InfoNCE', 'info_nce']\n",
    "\n",
    "\n",
    "class InfoNCE(nn.Module):\n",
    "    \"\"\"\n",
    "    Calculates the InfoNCE loss for self-supervised learning.\n",
    "    This contrastive loss enforces the embeddings of similar (positive) samples to be close\n",
    "        and those of different (negative) samples to be distant.\n",
    "    A query embedding is compared with one positive key and with one or more negative keys.\n",
    "\n",
    "    References:\n",
    "        https://arxiv.org/abs/1807.03748v2\n",
    "        https://arxiv.org/abs/2010.05113\n",
    "\n",
    "    Args:\n",
    "        temperature: Logits are divided by temperature before calculating the cross entropy.\n",
    "        reduction: Reduction method applied to the output.\n",
    "            Value must be one of ['none', 'sum', 'mean'].\n",
    "            See torch.nn.functional.cross_entropy for more details about each option.\n",
    "        negative_mode: Determines how the (optional) negative_keys are handled.\n",
    "            Value must be one of ['paired', 'unpaired'].\n",
    "            If 'paired', then each query sample is paired with a number of negative keys.\n",
    "            Comparable to a triplet loss, but with multiple negatives per sample.\n",
    "            If 'unpaired', then the set of negative keys are all unrelated to any positive key.\n",
    "\n",
    "    Input shape:\n",
    "        query: (N, D) Tensor with query samples (e.g. embeddings of the input).\n",
    "        positive_key: (N, D) Tensor with positive samples (e.g. embeddings of augmented input).\n",
    "        negative_keys (optional): Tensor with negative samples (e.g. embeddings of other inputs)\n",
    "            If negative_mode = 'paired', then negative_keys is a (N, M, D) Tensor.\n",
    "            If negative_mode = 'unpaired', then negative_keys is a (M, D) Tensor.\n",
    "            If None, then the negative keys for a sample are the positive keys for the other samples.\n",
    "\n",
    "    Returns:\n",
    "         Value of the InfoNCE Loss.\n",
    "\n",
    "     Examples:\n",
    "        >>> loss = InfoNCE()\n",
    "        >>> batch_size, num_negative, embedding_size = 32, 48, 128\n",
    "        >>> query = torch.randn(batch_size, embedding_size)\n",
    "        >>> positive_key = torch.randn(batch_size, embedding_size)\n",
    "        >>> negative_keys = torch.randn(num_negative, embedding_size)\n",
    "        >>> output = loss(query, positive_key, negative_keys)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature=0.1, reduction='mean', negative_mode='unpaired'):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.reduction = reduction\n",
    "        self.negative_mode = negative_mode\n",
    "\n",
    "    def forward(self, query, positive_key, negative_keys=None):\n",
    "        return info_nce(query, positive_key, negative_keys,\n",
    "                        temperature=self.temperature,\n",
    "                        reduction=self.reduction,\n",
    "                        negative_mode=self.negative_mode)\n",
    "\n",
    "\n",
    "def info_nce(query, positive_key, negative_keys=None, temperature=0.1, reduction='mean', negative_mode='unpaired'):\n",
    "    # Check input dimensionality.\n",
    "    if query.dim() != 2:\n",
    "        raise ValueError('<query> must have 2 dimensions.')\n",
    "    if positive_key.dim() != 2:\n",
    "        raise ValueError('<positive_key> must have 2 dimensions.')\n",
    "    if negative_keys is not None:\n",
    "        if negative_mode == 'unpaired' and negative_keys.dim() != 2:\n",
    "            raise ValueError(\"<negative_keys> must have 2 dimensions if <negative_mode> == 'unpaired'.\")\n",
    "        if negative_mode == 'paired' and negative_keys.dim() != 3:\n",
    "            raise ValueError(\"<negative_keys> must have 3 dimensions if <negative_mode> == 'paired'.\")\n",
    "\n",
    "    # Check matching number of samples.\n",
    "    if len(query) != len(positive_key):\n",
    "        raise ValueError('<query> and <positive_key> must must have the same number of samples.')\n",
    "    if negative_keys is not None:\n",
    "        if negative_mode == 'paired' and len(query) != len(negative_keys):\n",
    "            raise ValueError(\"If negative_mode == 'paired', then <negative_keys> must have the same number of samples as <query>.\")\n",
    "\n",
    "    # Embedding vectors should have same number of components.\n",
    "    if query.shape[-1] != positive_key.shape[-1]:\n",
    "        raise ValueError('Vectors of <query> and <positive_key> should have the same number of components.')\n",
    "    if negative_keys is not None:\n",
    "        if query.shape[-1] != negative_keys.shape[-1]:\n",
    "            raise ValueError('Vectors of <query> and <negative_keys> should have the same number of components.')\n",
    "\n",
    "    # Normalize to unit vectors\n",
    "    query, positive_key, negative_keys = normalize(query, positive_key, negative_keys)\n",
    "    if negative_keys is not None:\n",
    "        # Explicit negative keys\n",
    "\n",
    "        # Cosine between positive pairs\n",
    "        positive_logit = torch.sum(query * positive_key, dim=1, keepdim=True)\n",
    "\n",
    "        if negative_mode == 'unpaired':\n",
    "            # Cosine between all query-negative combinations\n",
    "            negative_logits = query @ transpose(negative_keys)\n",
    "\n",
    "        elif negative_mode == 'paired':\n",
    "            query = query.unsqueeze(1)\n",
    "            negative_logits = query @ transpose(negative_keys)\n",
    "            negative_logits = negative_logits.squeeze(1)\n",
    "\n",
    "        # First index in last dimension are the positive samples\n",
    "        logits = torch.cat([positive_logit, negative_logits], dim=1)\n",
    "        labels = torch.zeros(len(logits), dtype=torch.long, device=query.device)\n",
    "    else:\n",
    "        # Negative keys are implicitly off-diagonal positive keys.\n",
    "\n",
    "        # Cosine between all combinations\n",
    "        logits = query @ transpose(positive_key)\n",
    "\n",
    "        # Positive keys are the entries on the diagonal\n",
    "        labels = torch.arange(len(query), device=query.device)\n",
    "\n",
    "    return F.cross_entropy(logits / temperature, labels, reduction=reduction)\n",
    "\n",
    "\n",
    "def transpose(x):\n",
    "    return x.transpose(-2, -1)\n",
    "\n",
    "\n",
    "def normalize(*xs):\n",
    "    return [None if x is None else F.normalize(x, dim=-1) for x in xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "temperature = 0.1\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "\n",
    "batch = torch.randn([10, 32])\n",
    "N= 10\n",
    "\n",
    "norm_batch = F.normalize(batch, p=2,dim=1)\n",
    "sim = torch.mm(norm_batch, norm_batch.t())\n",
    "print(sim)\n",
    "pos_sim = torch.exp(torch.diag(sim) / temperature)\n",
    "print(pos_sim)\n",
    "\n",
    "tri_mask = torch.ones(N, N, dtype=torch.bool)\n",
    "tri_mask.fill_diagonal_(False)\n",
    "neg = sim.masked_select(tri_mask).view(N, N - 1)\n",
    "all_sim = torch.exp(sim / temperature)\n",
    "\n",
    "logits = torch.sum(pos_sim) / torch.sum(all_sim, dim=1)\n",
    "\n",
    "lbl = torch.ones(N)\n",
    "loss = criterion(logits, lbl)\n",
    "\n",
    "mean_sim = torch.mean(torch.diag(sim))\n",
    "mean_neg = torch.mean(neg)\n",
    "print(f'loss : {loss}, mean_sim : {mean_sim}, mean_neg : {mean_neg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = InfoNCE(negative_mode='paired')\n",
    "batch = torch.randn([10, 1])\n",
    "\n",
    "def sampler(batch):\n",
    "    query = batch[:-1]\n",
    "    positive_pair = batch[1:]\n",
    "    negative_pair = []\n",
    "\n",
    "    for i in range(len(batch)-1):\n",
    "        neg_i = []\n",
    "        for j in range(len(batch)):\n",
    "            if i!=j and j!=i+1:\n",
    "                neg_i.append(batch[j])\n",
    "        neg_i = torch.stack(neg_i)\n",
    "        negative_pair.append(neg_i)\n",
    "\n",
    "    negative_pair = torch.stack(negative_pair)\n",
    "    \n",
    "    return query, positive_pair, negative_pair\n",
    "\n",
    "res = loss(query, positive_pair, negative_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.1602)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
